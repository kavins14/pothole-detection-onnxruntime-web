{
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7309233,
          "sourceType": "datasetVersion",
          "datasetId": 3887050
        }
      ],
      "dockerImageVersionId": 30627,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Pothole Segmentation for Road Damage Assessment",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavins14/pothole-detection-onnxruntime-web/blob/main/Pothole_Segmentation_for_Road_Damage_Assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "farzadnekouei_pothole_image_segmentation_dataset_path = kagglehub.dataset_download('farzadnekouei/pothole-image-segmentation-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "fp7RQIHy8WYa"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/FarzadNekouee/YOLOv8_Pothole_Segmentation_Road_Damage_Assessment/blob/master/images/cover_image_modified.png?raw=true\" width=\"2400\">"
      ],
      "metadata": {
        "id": "g2Okoi7O8WYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h1 style=\"font-size:24px; font-family:calibri; color:#ed2f00;\"><b>üîç Instance Segmentation Overview</b></h1>\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\"> <strong>Instance segmentation</strong> advances <a href=\"https://www.kaggle.com/code/farzadnekouei/real-time-traffic-density-estimation-with-yolov8\" target=\"_blank\"><strong>object detection</strong></a> by pinpointing and outlining the exact shapes of individual objects in an image. Outputs include precise masks for each object, accompanied by class labels and confidence scores. Critical for tasks needing detailed object contours, the leading models for this sophisticated process are <strong>U-Net</strong>, <strong>SegNet</strong>, <strong>DeepLabv3</strong>, and <strong>YOLOv8-seg</strong>, offering high precision in real-time applications.</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "87mW1G_68WYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h1 style=\"font-size:24px; font-family:calibri; color:#ed2f00;\"><b>üåü YOLOv8-seg: Precision Instance Segmentation</b></h1>\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Launched by <strong>Ultralytics</strong> in <strong>November 2023</strong>, the <strong>YOLOv8-seg</strong> models, denoted by the '-seg' suffix such as `yolov8n-seg.pt`, represent the forefront of instance segmentation, pretrained on the comprehensive <strong>COCO dataset</strong>. These models extend the robust object detection capabilities of YOLOv8 to provide highly precise segmentation, outlining each object in detail. Available in various sizes from 'n' to 'x', the models balance speed with accuracy, with the 'x' variant reaching a <strong>mask mean Average Precision of 43.4</strong> on COCO benchmark dataset. YOLOv8-seg models are ideal for applications requiring detailed spatial understanding of objects.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "6JbFGtpB8WYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h1 style=\"font-size:24px; font-family:calibri; color:#ed2f00;\"><b>üõ£Ô∏è Pothole Segmentation for Road Damage Assessment Project Overview</b></h1>\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        This project employs <strong>YOLOv8-seg</strong> to automate the detection and analysis of road damage, focusing on  <strong>potholes</strong>. By accurately identifying and measuring the extent of potholes, the project aims to enhance road repair initiatives and provide real-time hazard alerts, significantly improving road safety and contributing to smart urban planning. The fusion of computer vision with critical road maintenance efforts marks a significant stride in the evolution of autonomous vehicle navigation and technological infrastructure development.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "vfiP4vSX8WYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h1 style=\"font-size:24px; font-family:calibri; color:#ed2f00;\"><b>üéØ Project Objectives</b></h1>\n",
        "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
        "        <li><strong>Choosing YOLOv8n-seg for Speed:</strong> Opt for the YOLOv8n-seg model due to its balance of speed and accuracy, suitable for real-time pothole segmentation.</li>\n",
        "        <li><strong>Dataset Preparation for Model Fine-tuning:</strong> Compile and prepare a specific dataset of pothole images for model training and validation, incorporating various augmentations.</li>\n",
        "        <li><strong>Fine-Tuning YOLOv8-seg:</strong> Implement transfer learning to adapt the pre-trained YOLOv8-seg model for effective pothole detection and segmentation.</li>\n",
        "        <li><strong>Model Performance Evaluation:</strong> Conduct a comprehensive evaluation of the model, involving various analytical approaches to ensure accuracy and reliability.</li>\n",
        "        <li><strong>Model Inference:</strong> Evaluate the model's performance on validation images and a new test video to ensure its effectiveness in real-world scenarios.</li>\n",
        "        <li><strong>Real-Time Road Damage Assessment:</strong> Apply the model to real-time video data, aiming to continuously estimate the area and percentage of road damage caused by potholes.</li>\n",
        "    </ul>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "dSq2Fw_M8WYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"contents_tabel\"></a>   \n",
        "\n",
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 15px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h1 style=\"font-size:24px; font-family:calibri; color:#ed2f00;\"><b>üìã Table of Contents</b></h1>\n",
        "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
        "        <li><a href=\"#Initialization\" style=\"text-decor\n",
        "            ation: none;\">Step 1 | Setup and Initialization</a></li>\n",
        "        <li><a href=\"#Load_Model\" style=\"text-decoration: none;\">Step 2 | Loading YOLOv8-seg Pre-trained Model</a></li>\n",
        "        <li><a href=\"#Dataset_Exploration\" style=\"text-decoration: none;\">Step 3 | Dataset Exploration</a></li>\n",
        "        <li><a href=\"#Fine_Tuning\" style=\"text-decoration: none;\">Step 4 | Fine-Tuning YOLOv8-seg</a></li>\n",
        "        <li><a href=\"#Model_Performance\" style=\"text-decoration: none;\">Step 5 | Model Performance Evaluation</a>\n",
        "            <ul>\n",
        "                <li><a href=\"#Learning_Curves\" style=\"text-decoration: none;\">Step 5.1 | Learning Curves Analysis</a></li>\n",
        "                <li><a href=\"#Confidence_Threshold_Metrics\" style=\"text-decoration: none;\">Step 5.2 | Confidence Threshold Metrics Analysis</a></li>\n",
        "                <li><a href=\"#Precision_Recall_Curve\" style=\"text-decoration: none;\">Step 5.3 | Precision-Recall Curve Analysis</a></li>\n",
        "                <li><a href=\"#Confusion_Matrix\" style=\"text-decoration: none;\">Step 5.4 | Confusion Matrix Analysis</a></li>\n",
        "                <li><a href=\"#Performance_Metrics\" style=\"text-decoration: none;\">Step 5.5 | Validation Performance Metrics Assessment</a></li>\n",
        "            </ul>\n",
        "        </li>\n",
        "        <li><a href=\"#Model_Inference\" style=\"text-decoration: none;\">Step 6 | Model Inference</a></li>\n",
        "            <ul>\n",
        "                <li><a href=\"#Inference_Images\" style=\"text-decoration: none;\">Step 6.1 | Inference on Validation Images</a></li>\n",
        "                <li><a href=\"#Inference_Video\" style=\"text-decoration: none;\">Step 6.2 | Inference on a New Test Video</a></li>\n",
        "            </ul>\n",
        "        <li><a href=\"#Road_Damage_Assessment\" style=\"text-decoration: none;\">Step 7 | Real-Time Road Damage Assessment</a></li>\n",
        "    </ul>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "uIlx0_HA8WYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 align=\"left\"><font color=#ed2f00>Let's get started:</font></h2>"
      ],
      "metadata": {
        "id": "YhMViwlR8WYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Initialization\"></a>\n",
        "# <p style=\"background-color: #ed2f00; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 1 | Setup and Initialization</p>\n",
        "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "id": "lNftqtl_8WYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\"><strong>YOLOv8-seg</strong>, launched by <strong>Ultralytics</strong>, is accessible for easy installation via pip. Let's start by installing the Ultralytics package:</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "MASvSDr58WYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Ultralytics library\n",
        "!pip install ultralytics"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-01-07T17:58:57.135215Z",
          "iopub.execute_input": "2024-01-07T17:58:57.135997Z",
          "iopub.status.idle": "2024-01-07T17:59:10.607782Z",
          "shell.execute_reply.started": "2024-01-07T17:58:57.135962Z",
          "shell.execute_reply": "2024-01-07T17:59:10.606853Z"
        },
        "trusted": true,
        "id": "-9GAd5QR8WYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Next, I will import all necessary libraries required for our project:</p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "lkDQrjUd8WYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable warnings in the notebook to maintain clean output cells\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import cv2\n",
        "import yaml\n",
        "from PIL import Image\n",
        "from collections import deque\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import Video"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T17:59:10.610385Z",
          "iopub.execute_input": "2024-01-07T17:59:10.610869Z",
          "iopub.status.idle": "2024-01-07T17:59:15.000954Z",
          "shell.execute_reply.started": "2024-01-07T17:59:10.610834Z",
          "shell.execute_reply": "2024-01-07T17:59:15.000156Z"
        },
        "trusted": true,
        "id": "6E1KBSPF8WYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the visual appearance of Seaborn plots\n",
        "sns.set(rc={'axes.facecolor': '#ffe4de'}, style='darkgrid')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T17:59:15.005184Z",
          "iopub.execute_input": "2024-01-07T17:59:15.006023Z",
          "iopub.status.idle": "2024-01-07T17:59:15.011599Z",
          "shell.execute_reply.started": "2024-01-07T17:59:15.005986Z",
          "shell.execute_reply": "2024-01-07T17:59:15.010371Z"
        },
        "trusted": true,
        "id": "Bk_dU1u_8WYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Load_Model\"></a>\n",
        "# <p style=\"background-color: #ed2f00; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 2 | Loading YOLOv8-seg Pre-trained Model</p>\n",
        "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "id": "HBBhK5_N8WYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Here are the YOLOv8 pretrained Segment models. They come in various configurations, each optimized for speed and accuracy, and are pretrained on the COCO dataset, which includes <a href=\"https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml\" target=\"_blank\">80 diverse object categories</a>:</p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "ZGHkkIJk8WYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/FarzadNekouee/YOLOv8_Pothole_Segmentation_Road_Damage_Assessment/blob/master/images/YOLOv8_segmentation_models.png?raw=true\" width=\"2400\">"
      ],
      "metadata": {
        "id": "2Fz3GcCL8WYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h1 style=\"font-size:24px; font-family:calibri; color:#ed2f00;\"><b>üöÄ Choosing YOLOv8n-seg for Speed</b></h1>\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        The <strong>YOLOv8-seg</strong> lineup ranges from nano to xlarge, each striking a different balance between accuracy and speed. While larger models offer increased precision, they do so at the expense of response time. For our Pothole Segmentation task, which demands prompt processing, the <strong>YOLOv8n-seg</strong> is our chosen model. It's tailored for the fastest inference, making it highly suitable for real-time applications:\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "08v7wAWg8WYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained YOLOv8 nano segmentation model\n",
        "model = YOLO('yolov8n-seg.pt')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T17:59:15.012773Z",
          "iopub.execute_input": "2024-01-07T17:59:15.013117Z",
          "iopub.status.idle": "2024-01-07T17:59:15.766561Z",
          "shell.execute_reply.started": "2024-01-07T17:59:15.013086Z",
          "shell.execute_reply": "2024-01-07T17:59:15.765651Z"
        },
        "trusted": true,
        "id": "35K2ndzy8WYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        After loading the YOLOv8 nano segmentation model, it's important to note that '<strong>pothole</strong>' is <strong>not</strong> one of the 80 object categories the model was initially trained to recognize in the COCO dataset. To identify potholes, we'll need to fine-tune the model on a specialized dataset of pothole images for precise segmentation.\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "klSg4XWX8WYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Dataset_Exploration\"></a>\n",
        "# <p style=\"background-color: #ed2f00; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 3 | Dataset Exploration</p>\n",
        "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "id": "vezPIAwm8WYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h1 style=\"font-size:24px; font-family:calibri; color:#ed2f00;\"><b>üõ†Ô∏è Dataset Preparation for Model Fine-tuning</b></h1>\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        I've compiled a dataset specifically for training our pothole segmentation model, available on <a href=\"https://www.kaggle.com/datasets/farzadnekouei/pothole-image-segmentation-dataset\" target=\"_blank\">Kaggle</a>. This dataset includes 780 images, with 720 for training and 60 for validation. Each image has been prepared to align correctly and resized to <strong>640x640</strong> pixels. To enhance the model's learning, the training images have undergone various augmentations, such as flipping, cropping, rotating, shearing, and adjusting brightness and exposure. This thorough preparation ensures our model will learn to identify and segment potholes effectively.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "cvS4u4Aw8WYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h1 style=\"font-size:24px; font-family:calibri; color:#ed2f00;\"><b>üß© YOLOv8-seg Dataset Format</b></h1>\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        I've formatted our dataset specifically for the <strong>YOLOv8-seg</strong> model, prepared on <a href=\"https://universe.roboflow.com/farzad/pothole_segmentation_yolov8/dataset/1\">Roboflow</a>. It includes all elements essential for training a segmentation model effectively. Here is a breakdown:\n",
        "    </p>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        <b>1Ô∏è‚É£ <code>train</code> and <code>valid</code> directories:</b><br>\n",
        "        Our dataset is split into <code>train</code> and <code>valid</code> directories, each with <code>images</code> and <code>labels</code> subdirectories. The <code>train</code> contains 720 image-label pairs, while <code>valid</code> has 60. Label files detail the class index and normalized coordinates for each pothole instance in the <code>[class-index] [x1] [y1] [x2] [y2] ... [xn] [yn] </code> format. In this format, <code>[class-index]</code> is the index of the class for the object, and  <code>[x1] [y1] [x2] [y2] ... [xn] [yn]</code> are the bounding coordinates of the object's segmentation mask. The coordinates are separated by spaces.\n",
        "    </p>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        <b>2Ô∏è‚É£ <code>data.yaml</code>:</b><br>\n",
        "        The <code>data.yaml</code> file which is the Ultralytics YOLO dataset configuration file.  It specifies paths to the training and validation datasets, defines the number of classes (1), and the class name (‚ÄòPothole‚Äô). This format is crucial for setting up and training the model accurately with our dataset. Let's review the <code>data.yaml</code> to understand the setup:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "XyMypclA8WYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset_path\n",
        "dataset_path = '/kaggle/input/pothole-image-segmentation-dataset/Pothole_Segmentation_YOLOv8'\n",
        "\n",
        "# Set the path to the YAML file\n",
        "yaml_file_path = os.path.join(dataset_path, 'data.yaml')\n",
        "\n",
        "# Load and print the contents of the YAML file\n",
        "with open(yaml_file_path, 'r') as file:\n",
        "    yaml_content = yaml.load(file, Loader=yaml.FullLoader)\n",
        "    print(yaml.dump(yaml_content, default_flow_style=False))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T17:59:15.769675Z",
          "iopub.execute_input": "2024-01-07T17:59:15.770063Z",
          "iopub.status.idle": "2024-01-07T17:59:15.783088Z",
          "shell.execute_reply.started": "2024-01-07T17:59:15.770027Z",
          "shell.execute_reply": "2024-01-07T17:59:15.782233Z"
        },
        "trusted": true,
        "id": "3aiegxwj8WYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h1 style=\"font-size:24px; font-family:calibri; color:#ed2f00;\"><b>üóÇÔ∏è Understanding the data.yaml File</b></h1>\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        The <code>data.yaml</code> file is crucial for configuring our model; it locates the training (<code>train: ../train/images</code>) and validation (<code>val: ../valid/images</code>) images and specifies that we're detecting a single class (<code>nc: 1</code>), <strong>Pothole</strong>. This setup ensures our <strong>YOLOv8-seg</strong> model is finely tuned to identify and segment potholes from our custom dataset. It's the guide that helps our model learn precisely what to look for during the segmentation task.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "45EQONfW8WYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Now, let's continue our exploration by counting the images in both the training and validation sets and verifying their sizes:\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "Gnh49HIF8WYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set paths for training and validation image sets\n",
        "train_images_path = os.path.join(dataset_path, 'train', 'images')\n",
        "valid_images_path = os.path.join(dataset_path, 'valid', 'images')\n",
        "\n",
        "# Initialize counters for the number of images\n",
        "num_train_images = 0\n",
        "num_valid_images = 0\n",
        "\n",
        "# Initialize sets to hold the unique sizes of images\n",
        "train_image_sizes = set()\n",
        "valid_image_sizes = set()\n",
        "\n",
        "# Check train images sizes and count\n",
        "for filename in os.listdir(train_images_path):\n",
        "    if filename.endswith('.jpg'):\n",
        "        num_train_images += 1\n",
        "        image_path = os.path.join(train_images_path, filename)\n",
        "        with Image.open(image_path) as img:\n",
        "            train_image_sizes.add(img.size)\n",
        "\n",
        "# Check validation images sizes and count\n",
        "for filename in os.listdir(valid_images_path):\n",
        "    if filename.endswith('.jpg'):\n",
        "        num_valid_images += 1\n",
        "        image_path = os.path.join(valid_images_path, filename)\n",
        "        with Image.open(image_path) as img:\n",
        "            valid_image_sizes.add(img.size)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Number of training images: {num_train_images}\")\n",
        "print(f\"Number of validation images: {num_valid_images}\")\n",
        "\n",
        "# Check if all images in training set have the same size\n",
        "if len(train_image_sizes) == 1:\n",
        "    print(f\"All training images have the same size: {train_image_sizes.pop()}\")\n",
        "else:\n",
        "    print(\"Training images have varying sizes.\")\n",
        "\n",
        "# Check if all images in validation set have the same size\n",
        "if len(valid_image_sizes) == 1:\n",
        "    print(f\"All validation images have the same size: {valid_image_sizes.pop()}\")\n",
        "else:\n",
        "    print(\"Validation images have varying sizes.\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T17:59:15.784062Z",
          "iopub.execute_input": "2024-01-07T17:59:15.784317Z",
          "iopub.status.idle": "2024-01-07T17:59:19.024898Z",
          "shell.execute_reply.started": "2024-01-07T17:59:15.784295Z",
          "shell.execute_reply": "2024-01-07T17:59:19.023964Z"
        },
        "trusted": true,
        "id": "naBGaLhg8WYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h1 style=\"font-size:24px; font-family:calibri; color:#ed2f00;\"><b>üìä Dataset Analysis Insights</b></h1>\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        For our Pothole Segmentation project, the dataset comprises 720 training images and 60 validation images. All images are consistently sized at 640x640 pixels, matching the YOLOv8-seg model's input specifications. This uniformity ensures that the model trains and validates efficiently. The training to validation split provides a robust dataset for learning and a sufficient number for validation to assess the model's performance accurately.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "aGzdSVAG8WYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Finally at this step, let's take a look at a few images from the dataset to get a sense of what the data looks like:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "wfA_7odP8WYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the seed for the random number generator\n",
        "random.seed(0)\n",
        "\n",
        "# Create a list of image files\n",
        "image_files = [f for f in os.listdir(train_images_path) if f.endswith('.jpg')]\n",
        "\n",
        "# Randomly select 15 images\n",
        "random_images = random.sample(image_files, 15)\n",
        "\n",
        "# Create a new figure\n",
        "plt.figure(figsize=(19, 12))\n",
        "\n",
        "# Loop through each image and display it in a 3x5 grid\n",
        "for i, image_file in enumerate(random_images):\n",
        "    image_path = os.path.join(train_images_path, image_file)\n",
        "    image = Image.open(image_path)\n",
        "    plt.subplot(3, 5, i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "\n",
        "# Add a suptitle\n",
        "plt.suptitle('Random Selection of Dataset Images', fontsize=24)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Deleting unnecessary variable to free up memory\n",
        "del image_files"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T17:59:19.026085Z",
          "iopub.execute_input": "2024-01-07T17:59:19.026381Z",
          "iopub.status.idle": "2024-01-07T17:59:22.907535Z",
          "shell.execute_reply.started": "2024-01-07T17:59:19.026355Z",
          "shell.execute_reply": "2024-01-07T17:59:22.906389Z"
        },
        "trusted": true,
        "id": "CaqkOLKe8WYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Fine_Tuning\"></a>\n",
        "# <p style=\"background-color: #ed2f00; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 4 | Fine-Tuning YOLOv8-seg</p>\n",
        "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "id": "_fset7aW8WYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        In this step, I'll use transfer learning to fine-tune the <strong>YOLOv8-seg model</strong>, initially trained on the <strong>COCO dataset</strong>, for our <a href=\"https://www.kaggle.com/datasets/farzadnekouei/pothole-image-segmentation-dataset\">Pothole Image Segmentation Dataset</a>. Instead of starting the training from scratch with random weights, which requires a lot of data and time, I'll start with a model that already knows how to recognize various objects. This way, we save time and make the most out of our smaller dataset to teach the model how to identify and segment potholes in road images:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "CpEDfcMd8WYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on our custom dataset\n",
        "results = model.train(\n",
        "    data=yaml_file_path,     # Path to the dataset configuration file\n",
        "    epochs=150,              # Number of epochs to train for\n",
        "    imgsz=640,               # Size of input images as integer\n",
        "    patience=15,             # Epochs to wait for no observable improvement for early stopping of training\n",
        "    batch=16,                # Number of images per batch\n",
        "    optimizer='auto',        # Optimizer to use, choices=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto]\n",
        "    lr0=0.0001,              # Initial learning rate\n",
        "    lrf=0.01,                # Final learning rate (lr0 * lrf)\n",
        "    dropout=0.25,            # Use dropout regularization\n",
        "    device=0,                # Device to run on, i.e. cuda device=0\n",
        "    seed=42                  # Random seed for reproducibility\n",
        ")"
      ],
      "metadata": {
        "_kg_hide-output": false,
        "execution": {
          "iopub.status.busy": "2024-01-07T17:59:22.909109Z",
          "iopub.execute_input": "2024-01-07T17:59:22.909423Z",
          "iopub.status.idle": "2024-01-07T18:27:16.564293Z",
          "shell.execute_reply.started": "2024-01-07T17:59:22.909391Z",
          "shell.execute_reply": "2024-01-07T18:27:16.563291Z"
        },
        "trusted": true,
        "id": "TEBVGI4J8WYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Model_Performance\"></a>\n",
        "# <p style=\"background-color: #ed2f00; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\"> Step 5 | Model Performance Evaluation</p>\n",
        "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "id": "4wddwbEA8WYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        After training our pothole-detecting model, it created several output files. These files show different results and details from the training. Let's check out what files we have:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "BRva3atV8WYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the directory\n",
        "post_training_files_path = '/kaggle/working/runs/segment/train'\n",
        "\n",
        "# List the files in the directory\n",
        "!ls {post_training_files_path}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T18:27:16.566383Z",
          "iopub.execute_input": "2024-01-07T18:27:16.566683Z",
          "iopub.status.idle": "2024-01-07T18:27:17.55688Z",
          "shell.execute_reply.started": "2024-01-07T18:27:16.566656Z",
          "shell.execute_reply": "2024-01-07T18:27:17.555622Z"
        },
        "trusted": true,
        "id": "KLJ2lDL58WYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#ed2f00;\"><b>üìÅ Training Output Files Explanations</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        After training our model, we have these output files:\n",
        "    </p>\n",
        "    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n",
        "        <li><b>BoxF1_curve.png, MaskF1_curve.png:</b> These images show the F1 score over different confidence thresholds for bounding box and mask predictions respectively, indicating the balance between precision and recall.</li>\n",
        "        <li><b>BoxP_curve.png, MaskP_curve.png:</b> These graphs display the precision of the bounding box and mask predictions of the model as confidence levels vary.</li>\n",
        "        <li><b>BoxPR_curve.png, MaskPR_curve.png:</b> These are the precision-recall curves for bounding box and mask predictions, useful for seeing the trade-off between precision and recall for different confidence thresholds.</li>\n",
        "        <li><b>BoxR_curve.png, MaskR_curve.png:</b> These images show how recall for bounding box and mask predictions changes with different confidence levels.</li>\n",
        "        <li><b>confusion_matrix.png:</b> Illustrates the model's performance by showing the true versus predicted classifications for bounding boxes.</li>\n",
        "        <li><b>confusion_matrix_normalized.png:</b> A normalized version of the confusion matrix that represents the proportion of each class predictions relative to the total number of predictions.</li>\n",
        "        <li><b>labels.jpg:</b> It displays data distributions for detected objects by the model.</li>\n",
        "        <li><b>labels_correlogram.jpg:</b> A correlogram that analyzes how different predicted labels correlate with each other.</li>\n",
        "        <li><b>results.csv:</b> This csv file captures a comprehensive set of performance metrics recorded at each epoch during the model's training process.</li>\n",
        "        <li><b>results.png:</b> A complex graph showing various training and validation losses (box, segmentation, classification, and distribution focal losses), as well as precision and recall metrics. The 'B' denotes bounding box and 'M' denotes mask predictions, including mean average precision (mAP) scores at different intersection over union (IoU) thresholds.</li>\n",
        "        <li><b>train_batch0.jpg, train_batch1.jpg, train_batch2.jpg:</b> Sample images from the training dataset batches with model predictions overlaid for visual inspection.</li>\n",
        "        <li><b>val_batch0_labels.jpg, val_batch1_labels.jpg:</b> Validation set images with the true labels overlaid for comparison against the model's predictions.</li>\n",
        "        <li><b>val_batch0_pred.jpg, val_batch1_pred.jpg:</b> Validation set images with the model's predictions overlaid for evaluation.</li>\n",
        "        <li><b>weights folder:</b> Contains the <code>best.pt</code> and <code>last.pt</code> files, which are the best and most recent weights of our trained model respectively.</li>\n",
        "        <li><b>args.yaml:</b> A configuration file that contains the hyperparameters and settings used during training.</li>\n",
        "        <li><b>events.out.tfevents.*:</b> A TensorBoard log file that stores the events like losses and metrics that occurred during training.</li>\n",
        "    </ul>\n",
        "</div>"
      ],
      "metadata": {
        "id": "jDmjb3L88WYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        In the following, I will carry out a detailed examination and assessment of our model's performance, involving:\n",
        "    </p>\n",
        "    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n",
        "        <li><b>Learning Curves Analysis</b></li>\n",
        "        <li><b>Confidence Threshold Metrics Analysis</b></li>\n",
        "        <li><b>Precision-Recall Curve Analysis</b></li>\n",
        "        <li><b>Confusion Matrix Analysis</b></li>\n",
        "        <li><b>Validation Performance Metrics Assessment</b></li>\n",
        "    </ul>\n",
        "</div>"
      ],
      "metadata": {
        "id": "qPhELOzH8WYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Learning_Curves\"></a>\n",
        "# <b><span style='color:#ffbaab'>Step 5.1 |</span><span style='color:#ed2f00'> Learning Curves Analysis</span></b>\n",
        "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "id": "r5hiIKtb8WYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        In this step, I'll examine the trends in training and validation losses over epochs to evaluate how well our segmentation model is learning. These loss trends are essential indicators of the model's learning progress and can reveal if the model is improving, overfitting, or underfitting. The <code>results.png</code> file captures these trends and will be our reference. Let's first dive into this graph:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "Jg1G_guB8WYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the full file path by joining the directory path with the filename\n",
        "results_file_path = os.path.join(post_training_files_path, 'results.png')\n",
        "\n",
        "# Read the image using cv2\n",
        "image = cv2.imread(results_file_path)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display the image using matplotlib\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.imshow(image)\n",
        "plt.title('Training and Validation Loss Trends', fontsize=24)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T18:27:17.558573Z",
          "iopub.execute_input": "2024-01-07T18:27:17.558906Z",
          "iopub.status.idle": "2024-01-07T18:27:18.537671Z",
          "shell.execute_reply.started": "2024-01-07T18:27:17.558879Z",
          "shell.execute_reply": "2024-01-07T18:27:18.536727Z"
        },
        "trusted": true,
        "id": "cpogGU2I8WYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#ed2f00;\"><b>üìä Learning Curves Analysis</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        The <code>results.png</code> file contains vital visual data tracking the model's learning progress across various metrics over epochs:\n",
        "    </p>\n",
        "    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n",
        "        <li><b>train/box_loss, val/box_loss:</b> These charts track the model's bounding box loss during training and validation, indicating how accurately the model is able to predict the boxes over time.</li>\n",
        "        <li><b>train/seg_loss, val/seg_loss:</b> Here, we see the segmentation loss, which shows the model's accuracy in segmenting the images throughout the training and validation phases.</li>\n",
        "        <li><b>train/cls_loss, val/cls_loss:</b> These plots reveal the classification loss, illustrating the model's ability to correctly classify the objects within the bounding boxes.</li>\n",
        "        <li><b>train/dfl_loss, val/dfl_loss:</b> These charts represent the distribution focal loss, a measure of how well the model is learning to differentiate and accurately classify various objects and segments in the images, particularly focusing on the challenging or hard-to-classify cases.\n",
        "</li>\n",
        "        <li><b>metrics/precision(B), metrics/precision(M):</b> Show precision metrics for bounding boxes (B) and masks (M), reflecting the proportion of correct positive predictions made by the model.</li>\n",
        "        <li><b>metrics/recall(B), metrics/recall(M):</b> Indicate recall metrics, which assess the model's ability to detect all relevant instances in the dataset.</li>\n",
        "        <li><b>metrics/mAP50(B), metrics/mAP50-95(B):</b> Mean Average Precision at IOU=0.50 and across IOU=0.50-0.95 for bounding box predictions, providing a single-figure summary of accuracy.</li>\n",
        "        <li><b>metrics/mAP50(M), metrics/mAP50-95(M):</b> Similar to the bounding box metrics, these measure the model's mask prediction accuracy at specific IOU thresholds.</li>\n",
        "    </ul>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "o04RLctk8WYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        To discern whether our model is progressing, overfitting, or underfitting, it's essential to examine the loss metrics ‚Äî box_loss, seg_loss, cls_loss, dfl_loss ‚Äî for both the training and validation datasets side by side. This comparison is made clearer by plotting the loss curves together. I'll utilize the <code>results.csv</code> file, which contains a detailed record of performance metrics at every epoch throughout the model's training, to redraw these informative curves:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "WyP4WU308WYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to plot learning curves for loss values\n",
        "def plot_learning_curve(df, train_loss_col, val_loss_col, title, ylim_range=[0,2]):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    sns.lineplot(data=df, x='epoch', y=train_loss_col, label='Train Loss', color='blue', linestyle='-', linewidth=2)\n",
        "    sns.lineplot(data=df, x='epoch', y=val_loss_col, label='Validation Loss', color='#ed2f00', linestyle='--', linewidth=2)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.ylim(ylim_range)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T18:27:18.538872Z",
          "iopub.execute_input": "2024-01-07T18:27:18.539227Z",
          "iopub.status.idle": "2024-01-07T18:27:18.54604Z",
          "shell.execute_reply.started": "2024-01-07T18:27:18.539199Z",
          "shell.execute_reply": "2024-01-07T18:27:18.545012Z"
        },
        "trusted": true,
        "id": "XetJoJjL8WYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the full file path for 'results.csv' using the directory path and file name\n",
        "results_csv_path = os.path.join(post_training_files_path, 'results.csv')\n",
        "\n",
        "# Load the CSV file from the constructed path into a pandas DataFrame\n",
        "df = pd.read_csv(results_csv_path)\n",
        "\n",
        "# Remove any leading whitespace from the column names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Plot the learning curves for each loss\n",
        "plot_learning_curve(df, 'train/box_loss', 'val/box_loss', 'Bounding Box Loss Learning Curve')\n",
        "plot_learning_curve(df, 'train/cls_loss', 'val/cls_loss', 'Classification Loss Learning Curve')\n",
        "plot_learning_curve(df, 'train/dfl_loss', 'val/dfl_loss', 'Distribution Focal Loss Learning Curve')\n",
        "plot_learning_curve(df, 'train/seg_loss', 'val/seg_loss', 'Segmentation Loss Learning Curve', ylim_range=[0,5])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T18:27:18.547461Z",
          "iopub.execute_input": "2024-01-07T18:27:18.548361Z",
          "iopub.status.idle": "2024-01-07T18:27:19.885018Z",
          "shell.execute_reply.started": "2024-01-07T18:27:18.548326Z",
          "shell.execute_reply": "2024-01-07T18:27:19.884118Z"
        },
        "trusted": true,
        "id": "MNcORNBL8WYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#ed2f00;\"><b>üîç Model's Learning State</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Our model's training progress, as shown by the learning curves, demonstrates a good fit on the validation data. The training and validation losses are close, which is a positive sign of the model's ability to generalize:\n",
        "    </p>\n",
        "    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n",
        "        <li><b>Bounding Box Loss:</b> The gap between training and validation loss suggests some overfitting, but the model is generally predicting boxes well.</li>\n",
        "        <li><b>Classification Loss:</b> Classification performance is strong on training data, with a slight drop on validation, indicating room for improvement.</li>\n",
        "        <li><b>Distribution Focal Loss:</b> Indicates focused learning on challenging examples, with validation performance suggesting further tuning may be beneficial.</li>\n",
        "        <li><b>Segmentation Loss:</b> Shows the model's capability in segmentation with some variability in validation loss, highlighting potential overfitting.</li>\n",
        "    </ul>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        While it's challenging to completely eliminate overfitting, the close alignment of the curves is an encouraging indicator of generalization. <strong>Adding more diverse data to our training set could further enhance the model's performance.</strong>\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "fhvnsXRE8WYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Confidence_Threshold_Metrics\"></a>\n",
        "# <b><span style='color:#ffbaab'>Step 5.2 |</span><span style='color:#ed2f00'> Confidence Threshold Metrics Analysis</span></b>\n",
        "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "id": "_QJrRM1g8WYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Now, I am going to focus on evaluating our model's predictive performance at various levels of confidence using precision, recall, and F1 score metrics for both bounding box and mask predictions:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "MaR-HWUy8WYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the filenames for 'Box' and 'Mask' metrics along with their titles\n",
        "box_files_titles = {\n",
        "    'BoxP_curve.png': 'Bounding Box Precision-Confidence Curve',\n",
        "    'BoxR_curve.png': 'Bounding Box Recall-Confidence Curve',\n",
        "    'BoxF1_curve.png': 'Bounding Box F1-Confidence Curve'\n",
        "}\n",
        "mask_files_titles = {\n",
        "    'MaskP_curve.png': 'Mask Precision-Confidence Curve',\n",
        "    'MaskR_curve.png': 'Mask Recall-Confidence Curve',\n",
        "    'MaskF1_curve.png': 'Mask F1-Confidence Curve'\n",
        "}\n",
        "\n",
        "# Create a 3x2 subplot\n",
        "fig, axs = plt.subplots(3, 2, figsize=(20, 20))\n",
        "\n",
        "# Function to read and convert image for plotting\n",
        "def read_and_convert_image(file_path):\n",
        "    # Read the image using cv2\n",
        "    image = cv2.imread(file_path)\n",
        "    # Convert from BGR to RGB\n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Plot 'Box' images in the first column with meaningful titles\n",
        "for i, (filename, title) in enumerate(box_files_titles.items()):\n",
        "    img_path = os.path.join(post_training_files_path, filename)\n",
        "    img = read_and_convert_image(img_path)\n",
        "    axs[i, 0].imshow(img)\n",
        "    axs[i, 0].set_title(title, fontsize=20)\n",
        "    axs[i, 0].axis('off')\n",
        "\n",
        "# Plot 'Mask' images in the second column with meaningful titles\n",
        "for i, (filename, title) in enumerate(mask_files_titles.items()):\n",
        "    img_path = os.path.join(post_training_files_path, filename)\n",
        "    img = read_and_convert_image(img_path)\n",
        "    axs[i, 1].imshow(img)\n",
        "    axs[i, 1].set_title(title, fontsize=20)\n",
        "    axs[i, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T18:27:19.88629Z",
          "iopub.execute_input": "2024-01-07T18:27:19.886582Z",
          "iopub.status.idle": "2024-01-07T18:27:24.404731Z",
          "shell.execute_reply.started": "2024-01-07T18:27:19.886556Z",
          "shell.execute_reply": "2024-01-07T18:27:24.403833Z"
        },
        "trusted": true,
        "id": "WBXesUnD8WYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#ed2f00;\"><b>üìà Confidence Threshold Metrics Analysis</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        The precision, recall, and F1 score confidence curves demonstrate outstanding performance in our segmentation model:\n",
        "    </p>\n",
        "    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n",
        "        <li><b>Precision-Confidence Curve:</b> Shows near-perfect precision across all confidence levels, indicating very accurate predictions for both bounding boxes and masks.</li>\n",
        "        <li><b>Recall-Confidence Curve:</b> Maintains high recall across all confidence thresholds, suggesting the model consistently identifies true positives.</li>\n",
        "        <li><b>F1-Confidence Curve:</b> Reveals a stable and high F1 score, suggesting a balanced precision and recall, and the model performs well even at higher confidence thresholds.</li>\n",
        "    </ul>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Overall, the model shows excellent generalization capabilities with robust predictive performance. The consistently high metrics across different confidence thresholds indicate that minimal adjustment is needed for practical application.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "ouuKh0N88WYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Precision_Recall_Curve\"></a>\n",
        "# <b><span style='color:#ffbaab'>Step 5.3 |</span><span style='color:#ed2f00'> Precision-Recall Curve Analysis</span></b>\n",
        "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "id": "8SU2g7_e8WYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Now, I am going to analyze the precision-recall curves for both bounding box and mask predictions:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "N8CY_Ndw8WYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the filenames for 'Box' and 'Mask' metrics along with their titles\n",
        "pr_files_titles = {\n",
        "    'BoxPR_curve.png': 'Bounding Box Precision-Recall Curve',\n",
        "    'MaskPR_curve.png': 'Mask Precision-Recall Curve'\n",
        "}\n",
        "\n",
        "# Create a 1x2 subplot\n",
        "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "# Plot 'Box' and 'Mask' images in the subplot with meaningful titles\n",
        "for i, (filename, title) in enumerate(pr_files_titles.items()):\n",
        "    img_path = os.path.join(post_training_files_path, filename)\n",
        "    img = read_and_convert_image(img_path)\n",
        "    axs[i].imshow(img)\n",
        "    axs[i].set_title(title, fontsize=20)\n",
        "    axs[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T18:27:24.40911Z",
          "iopub.execute_input": "2024-01-07T18:27:24.409423Z",
          "iopub.status.idle": "2024-01-07T18:27:26.005828Z",
          "shell.execute_reply.started": "2024-01-07T18:27:24.409394Z",
          "shell.execute_reply": "2024-01-07T18:27:26.004883Z"
        },
        "trusted": true,
        "id": "53XoIsn18WYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#ed2f00;\"><b>üìà Precision-Recall Curve Analysis</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        An examination of the precision-recall curves reveals key aspects of our segmentation model's performance:\n",
        "    </p>\n",
        "    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n",
        "        <li><b>Bounding Box Precision-Recall Curve:</b> The curve indicates a high level of precision, with a mean Average Precision (mAP) of 0.72, showing the model's effectiveness in accurately identifying objects.</li>\n",
        "        <li><b>Mask Precision-Recall Curve:</b> This curve suggests almost the same precision in segmenting objects, evidenced by a mAP of 0.72, pointing to the model's proficiency in mask predictions.</li>\n",
        "    </ul>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        The high precision across various recall levels for both bounding boxes and masks indicates that the model performs reliably and can be trusted for practical use.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "6pkGWIG18WYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Confusion_Matrix\"></a>\n",
        "# <b><span style='color:#ffbaab'>Step 5.4 |</span><span style='color:#ed2f00'> Confusion Matrix Analysis</span></b>\n",
        "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "id": "eHks7oQB8WYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Next, I will concentrate on presenting and thoroughly examining the confusion matrix that reflects our model's predictive results on the validation set:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "mt5DYwgi8WYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the path to the confusion matrix images\n",
        "confusion_matrix_path = os.path.join(post_training_files_path, 'confusion_matrix.png')\n",
        "confusion_matrix_normalized_path = os.path.join(post_training_files_path, 'confusion_matrix_normalized.png')\n",
        "\n",
        "# Create a 1x2 subplot\n",
        "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "# Read and convert both images\n",
        "cm_img = read_and_convert_image(confusion_matrix_path)\n",
        "cm_norm_img = read_and_convert_image(confusion_matrix_normalized_path)\n",
        "\n",
        "# Display the images\n",
        "axs[0].imshow(cm_img)\n",
        "axs[0].set_title('Confusion Matrix', fontsize=24)\n",
        "axs[0].axis('off')\n",
        "\n",
        "axs[1].imshow(cm_norm_img)\n",
        "axs[1].set_title('Normalized Confusion Matrix', fontsize=24)\n",
        "axs[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T18:27:26.007237Z",
          "iopub.execute_input": "2024-01-07T18:27:26.007623Z",
          "iopub.status.idle": "2024-01-07T18:27:28.542763Z",
          "shell.execute_reply.started": "2024-01-07T18:27:26.007588Z",
          "shell.execute_reply": "2024-01-07T18:27:28.541955Z"
        },
        "trusted": true,
        "id": "mnyJKO988WYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#ed2f00;\"><b>üîç Confusion Matrix Analysis</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        The normalized confusion matrix for our YOLOv8 pothole segmentation model provides valuable insights into its performance. The model has a 70% true positive rate, meaning it correctly identifies potholes 70% of the time. However, there is a 30% false negative rate, indicating that some potholes are missed during detection. Given the challenging nature of pothole segmentation, this performance is commendable.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "WdWNpfwk8WYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Performance_Metrics\"></a>\n",
        "# <b><span style='color:#ffbaab'>Step 5.5 |</span><span style='color:#ed2f00'> Validation Performance Metrics Assessment</span></b>\n",
        "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "id": "tutyjgwJ8WYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Finally, I am delving into various metrics to assess our best segmentation model's predictive capabilities on the validation set:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "oGj54RON8WYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the path to the best model weights file using os.path.join\n",
        "best_model_path = os.path.join(post_training_files_path, 'weights/best.pt')\n",
        "\n",
        "# Load the best model weights into the YOLO model\n",
        "best_model = YOLO(best_model_path)\n",
        "\n",
        "# Validate the best model using the validation set with default parameters\n",
        "metrics = best_model.val(split='val')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T18:27:28.543953Z",
          "iopub.execute_input": "2024-01-07T18:27:28.544255Z",
          "iopub.status.idle": "2024-01-07T18:27:40.335869Z",
          "shell.execute_reply.started": "2024-01-07T18:27:28.544228Z",
          "shell.execute_reply": "2024-01-07T18:27:40.334764Z"
        },
        "trusted": true,
        "id": "oVf-H8sN8WYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the dictionary to a pandas DataFrame and use the keys as the index\n",
        "metrics_df = pd.DataFrame.from_dict(metrics.results_dict, orient='index', columns=['Metric Value'])\n",
        "\n",
        "# Display the DataFrame\n",
        "metrics_df.round(3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T18:27:40.337697Z",
          "iopub.execute_input": "2024-01-07T18:27:40.338016Z",
          "iopub.status.idle": "2024-01-07T18:27:40.356474Z",
          "shell.execute_reply.started": "2024-01-07T18:27:40.337989Z",
          "shell.execute_reply": "2024-01-07T18:27:40.35561Z"
        },
        "trusted": true,
        "id": "G-MZCIjV8WYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <h2 style=\"font-size:22px; font-family:calibri; color:#ed2f00;\"><b>üìä Validation Performance Metrics Assessment</b></h2>\n",
        "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Our YOLOv8 <strong>tiny</strong> model for pothole segmentation demonstrates promising results in validation. Precision metrics for bounding boxes (B) and masks (M) show high values of 0.75 and 0.71, respectively, indicating accurate predictions. Recall scores are slightly lower, with 0.61 for bounding boxes and 0.66 for masks, suggesting some true potholes may be missed. The mAP50 scores are robust at 0.72 for both bounding box and mask predictions, but the mAP50-95 scores show there is room to improve the consistency of predictions across different IoU thresholds. The overall fitness score of 0.92 reflects a well-trained model with potential for fine-tuning.\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "AvoSAVCP8WYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Model_Inference\"></a>\n",
        "# <p style=\"background-color: #ed2f00; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\"> Step 6 | Model Inference</p>\n",
        "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "id": "-QWH6pAY8WYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        To effectively evaluate how well our model performs on new data, I'll carry out inferences in two key stages:\n",
        "    </p>\n",
        "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
        "        <li><b>Inference on Validation Images</b></li>\n",
        "        <li><b>Inference on a New Test Video</b></li>\n",
        "    </ul>\n",
        "</div>"
      ],
      "metadata": {
        "id": "BfljVLbu8WYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Inference_Images\"></a>\n",
        "# <b><span style='color:#ffbaab'>Step 6.1 |</span><span style='color:#ed2f00'> Inference on Validation Images</span></b>\n",
        "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "id": "RjRzQp_d8WYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        To begin, I will showcase the capabilities of our best segmentation model on a selection of images from the validation set:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "owcDunIl8WYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the validation images\n",
        "valid_images_path = os.path.join(dataset_path, 'valid', 'images')\n",
        "\n",
        "# List all jpg images in the directory\n",
        "image_files = [file for file in os.listdir(valid_images_path) if file.endswith('.jpg')]\n",
        "\n",
        "# Select 9 images at equal intervals\n",
        "num_images = len(image_files)\n",
        "selected_images = [image_files[i] for i in range(0, num_images, num_images // 9)]\n",
        "\n",
        "# Initialize the subplot\n",
        "fig, axes = plt.subplots(3, 3, figsize=(20, 21))\n",
        "fig.suptitle('Validation Set Inferences', fontsize=24)\n",
        "\n",
        "# Perform inference on each selected image and display it\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    image_path = os.path.join(valid_images_path, selected_images[i])\n",
        "    results = best_model.predict(source=image_path, imgsz=640)\n",
        "    annotated_image = results[0].plot()\n",
        "    annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
        "    ax.imshow(annotated_image_rgb)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T18:27:40.357659Z",
          "iopub.execute_input": "2024-01-07T18:27:40.358082Z",
          "iopub.status.idle": "2024-01-07T18:27:45.024074Z",
          "shell.execute_reply.started": "2024-01-07T18:27:40.358049Z",
          "shell.execute_reply": "2024-01-07T18:27:45.022422Z"
        },
        "trusted": true,
        "id": "CPW1DbFZ8WYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Inference_Video\"></a>\n",
        "# <b><span style='color:#ffbaab'>Step 6.2 |</span><span style='color:#ed2f00'> Inference on a New Test Video</span></b>\n",
        "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "id": "PM2To2wf8WYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Next, we'll see how well our model can handle a brand new test video it hasn't seen before. This is an important test to check if our model can be used in the real world, showing that it can do a good job even with new and different data:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "LwVSYzdp8WYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the sample video in the dataset\n",
        "dataset_video_path = '/kaggle/input/pothole-image-segmentation-dataset/Pothole_Segmentation_YOLOv8/sample_video.mp4'\n",
        "\n",
        "# Define the destination path in the working directory\n",
        "video_path = '/kaggle/working/sample_video.mp4'\n",
        "\n",
        "# Copy the video file from its original location in the dataset to the current working directory in Kaggle\n",
        "shutil.copyfile(dataset_video_path, video_path)\n",
        "\n",
        "# Initiate vehicle detection on the sample video using the best performing model and save the output\n",
        "best_model.predict(source=video_path, save=True)"
      ],
      "metadata": {
        "scrolled": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-01-07T18:27:45.02593Z",
          "iopub.execute_input": "2024-01-07T18:27:45.02641Z",
          "iopub.status.idle": "2024-01-07T18:27:58.563211Z",
          "shell.execute_reply.started": "2024-01-07T18:27:45.026363Z",
          "shell.execute_reply": "2024-01-07T18:27:58.562199Z"
        },
        "trusted": true,
        "id": "bauSYajf8WYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        To display the processed video in the notebook environment, I will convert the output <code>.avi</code> file into the widely supported <code>.mp4</code> format for better compatibility:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "l9l8Wfu48WYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the .avi video generated by the YOLOv8 prediction to .mp4 format for compatibility with notebook display\n",
        "!ffmpeg -y -loglevel panic -i /kaggle/working/runs/segment/predict/sample_video.avi processed_sample_video.mp4\n",
        "\n",
        "# Embed and display the processed sample video within the notebook\n",
        "Video(\"processed_sample_video.mp4\", embed=True, width=960)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T18:27:58.564493Z",
          "iopub.execute_input": "2024-01-07T18:27:58.564788Z",
          "iopub.status.idle": "2024-01-07T18:28:13.966821Z",
          "shell.execute_reply.started": "2024-01-07T18:27:58.564761Z",
          "shell.execute_reply": "2024-01-07T18:28:13.964883Z"
        },
        "trusted": true,
        "id": "IU4D1CqN8WYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        After confirming our model's generalization capabilities, let's proceed to save the model with its best-performing weights:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "O_gwV6OD8WYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the model\n",
        "best_model.export(format='onnx')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T18:28:13.969025Z",
          "iopub.execute_input": "2024-01-07T18:28:13.969537Z",
          "iopub.status.idle": "2024-01-07T18:28:16.45913Z",
          "shell.execute_reply.started": "2024-01-07T18:28:13.969472Z",
          "shell.execute_reply": "2024-01-07T18:28:16.458223Z"
        },
        "trusted": true,
        "id": "F7Ds0zDs8WYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"Road_Damage_Assessment\"></a>\n",
        "# <p style=\"background-color: #ed2f00; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\"> Step 7 | Real-Time Road Damage Assessment</p>\n",
        "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
      ],
      "metadata": {
        "id": "2cy1IFyM8WYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        As we move to the practical step of our project, I aim to deploy our final pothole segmentation model for <strong>real-time road damage assessment</strong>, particularly focusing on <strong>potholes</strong>. Our final goal will be to continuously calculate the area and percentage of pothole damage in each video frame. This accurate identification and quantification will support road repair strategies and provide immediate hazard warnings, enhancing road safety and aiding smart city development.\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "bpxuPluy8WYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Initially, I'll demonstrate our approach using a sample image, applying our best model to detect, and display potholes with segmented masks representing the damaged areas:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "wS1C_7K08WYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the validation images\n",
        "valid_images_path = os.path.join(dataset_path, 'valid', 'images')\n",
        "\n",
        "# List all jpg images in the directory\n",
        "image_files = [file for file in os.listdir(valid_images_path) if file.endswith('.jpg')]\n",
        "\n",
        "# Select a sample image\n",
        "selected_image = image_files[45]\n",
        "\n",
        "# Perform inference on the selected image\n",
        "image_path = os.path.join(valid_images_path, selected_image)\n",
        "results = best_model.predict(source=image_path, imgsz=640, conf=0.5)\n",
        "annotated_image = results[0].plot()\n",
        "annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Determine the number of subplots needed (1 original + number of masks)\n",
        "num_subplots = 1 + (len(results[0].masks.data) if results[0].masks is not None else 0)\n",
        "\n",
        "# Initialize the subplot with 1 row and n columns\n",
        "fig, axes = plt.subplots(1, num_subplots, figsize=(15, 5))\n",
        "\n",
        "# Display the original annotated image\n",
        "axes[0].imshow(annotated_image_rgb)\n",
        "axes[0].set_title('Original Image')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# If multiple masks, iterate and display each mask\n",
        "if results[0].masks is not None:\n",
        "    masks = results[0].masks.data.cpu().numpy()\n",
        "    for i, mask in enumerate(masks):\n",
        "        # Threshold the mask to make sure it's binary\n",
        "        # Any value greater than 0 is set to 255, else it remains 0\n",
        "        binary_mask = (mask > 0).astype(np.uint8) * 255\n",
        "        axes[i+1].imshow(binary_mask, cmap='gray')\n",
        "        axes[i+1].set_title(f'Segmented Mask {i+1}')\n",
        "        axes[i+1].axis('off')\n",
        "\n",
        "# Adjust layout and display the subplot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T18:28:16.460431Z",
          "iopub.execute_input": "2024-01-07T18:28:16.460717Z",
          "iopub.status.idle": "2024-01-07T18:28:17.401779Z",
          "shell.execute_reply.started": "2024-01-07T18:28:16.46069Z",
          "shell.execute_reply": "2024-01-07T18:28:17.40085Z"
        },
        "trusted": true,
        "id": "2dUaOxVA8WYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Next, I will outline each detected pothole with contours on the masks, calculate the area of each, and ultimately determine both the total damaged area and the percentage of road damage due to potholes:\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "gunLovvq8WYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables to hold total area and individual areas\n",
        "total_area = 0\n",
        "area_list = []\n",
        "\n",
        "# Set up the subplot for displaying masks\n",
        "fig, axes = plt.subplots(1, len(masks), figsize=(12, 8))\n",
        "\n",
        "# Perform operations if masks are available\n",
        "if results[0].masks is not None:\n",
        "    masks = results[0].masks.data.cpu().numpy()   # Retrieve masks as numpy arrays\n",
        "    image_area = masks.shape[1] * masks.shape[2]  # Calculate total number of pixels in the image\n",
        "    for i, mask in enumerate(masks):\n",
        "        binary_mask = (mask > 0).astype(np.uint8) * 255  # Convert mask to binary\n",
        "        color_mask = cv2.cvtColor(binary_mask, cv2.COLOR_GRAY2BGR)  # Convert binary mask to color\n",
        "        contours, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)  # Find contours in the binary mask\n",
        "        contour = contours[0]  # Retrieve the first contour\n",
        "        area = cv2.contourArea(contour)  # Calculate the area of the pothole\n",
        "        area_list.append(area)  # Append area to the list\n",
        "        cv2.drawContours(color_mask, [contour], -1, (0, 255, 0), 3)  # Draw the contour on the mask\n",
        "\n",
        "        # Display the mask with the green contour\n",
        "        axes[i].imshow(color_mask)\n",
        "        axes[i].set_title(f'Pothole {i+1}')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "# Display all masks\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate and print areas after displaying the images\n",
        "for i, area in enumerate(area_list):\n",
        "    print(f\"Area of Pothole {i+1}: {area} pixels\")\n",
        "    total_area += area  # Sum the areas for total\n",
        "\n",
        "# Calculate and print the total damaged area and percentage of road damaged by potholes\n",
        "print(\"-\"*50)\n",
        "print(f\"Total Damaged Area by Potholes: {total_area} pixels\")\n",
        "print(f\"Total Pixels in Image: {image_area} pixels\")\n",
        "print(f\"Percentage of Road Damaged: {(total_area / image_area) * 100:.2f}%\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T18:28:17.403395Z",
          "iopub.execute_input": "2024-01-07T18:28:17.403783Z",
          "iopub.status.idle": "2024-01-07T18:28:18.064896Z",
          "shell.execute_reply.started": "2024-01-07T18:28:17.403747Z",
          "shell.execute_reply": "2024-01-07T18:28:18.06396Z"
        },
        "trusted": true,
        "id": "PG0oglqF8WYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        So far, we've demonstrated how to apply our segmentation model to each image, extract masks for potholes, outline them with contours, and calculate each's area. By summing these areas, we estimate the road damage percentage caused by potholes. Next, I'll extend this approach to each frame of an unseen video, enabling real-time road damage assessment:\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "n1uejy_R8WYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the video path\n",
        "video_path = '/kaggle/working/sample_video.mp4'\n",
        "\n",
        "# Define font, scale, colors, and position for the annotation\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "font_scale = 1\n",
        "text_position = (40, 80)\n",
        "font_color = (255, 255, 255)    # White color for text\n",
        "background_color = (0, 0, 255)  # Red background for text\n",
        "\n",
        "# Initialize a deque with fixed length for averaging the last 10 percentage damages\n",
        "damage_deque = deque(maxlen=10)\n",
        "\n",
        "# Open the video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Define the codec and create VideoWriter object\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out = cv2.VideoWriter('road_damage_assessment.avi', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "# Read until video is completed\n",
        "while cap.isOpened():\n",
        "     # Capture frame-by-frame\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        # Perform inference on the frame\n",
        "        results = best_model.predict(source=frame, imgsz=640, conf=0.25)\n",
        "        processed_frame = results[0].plot(boxes=False)\n",
        "\n",
        "        # Initializes percentage_damage to 0\n",
        "        percentage_damage = 0\n",
        "\n",
        "        # If masks are available, calculate total damage area and percentage\n",
        "        if results[0].masks is not None:\n",
        "            total_area = 0\n",
        "            masks = results[0].masks.data.cpu().numpy()\n",
        "            image_area = frame.shape[0] * frame.shape[1]  # total number of pixels in the image\n",
        "            for mask in masks:\n",
        "                binary_mask = (mask > 0).astype(np.uint8) * 255\n",
        "                contour, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "                total_area += cv2.contourArea(contour[0])\n",
        "\n",
        "            percentage_damage = (total_area / image_area) * 100\n",
        "\n",
        "        # Calculate and update the percentage damage\n",
        "        damage_deque.append(percentage_damage)\n",
        "        smoothed_percentage_damage = sum(damage_deque) / len(damage_deque)\n",
        "\n",
        "        # Draw a thick line for text background\n",
        "        cv2.line(processed_frame, (text_position[0], text_position[1] - 10),\n",
        "                 (text_position[0] + 350, text_position[1] - 10), background_color, 40)\n",
        "\n",
        "        # Annotate the frame with the percentage of damage\n",
        "        cv2.putText(processed_frame, f'Road Damage: {smoothed_percentage_damage:.2f}%', text_position, font, font_scale, font_color, 2, cv2.LINE_AA)\n",
        "\n",
        "        # Write the processed frame to the output video\n",
        "        out.write(processed_frame)\n",
        "\n",
        "        # Uncomment the following 3 lines if running this code on a local machine to view the real-time processing results\n",
        "        # cv2.imshow('Road Damage Assessment', processed_frame) # Display the processed frame\n",
        "        # if cv2.waitKey(1) & 0xFF == ord('q'): # Press Q on keyboard to exit the loop\n",
        "        #     break\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Release the video capture and video write objects\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# Close all the frames\n",
        "# cv2.destroyAllWindows()"
      ],
      "metadata": {
        "scrolled": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-01-07T18:28:18.066746Z",
          "iopub.execute_input": "2024-01-07T18:28:18.067142Z",
          "iopub.status.idle": "2024-01-07T18:28:30.528343Z",
          "shell.execute_reply.started": "2024-01-07T18:28:18.067105Z",
          "shell.execute_reply": "2024-01-07T18:28:30.527528Z"
        },
        "trusted": true,
        "id": "yZ-64cGA8WYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:#ffe4de; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
        "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
        "        Finally, lets convert the output <code>.avi</code> video to <code>.mp4</code> format for notebook playback and display the result:\n",
        "    </p>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "Ch99KqnE8WYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the .avi video generated by our traffic density estimation app to .mp4 format for compatibility with notebook display\n",
        "!ffmpeg -y -loglevel panic -i /kaggle/working/road_damage_assessment.avi road_damage_assessment.mp4\n",
        "\n",
        "# Embed and display the processed sample video within the notebook\n",
        "Video(\"road_damage_assessment.mp4\", embed=True, width=960)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-07T18:28:30.529786Z",
          "iopub.execute_input": "2024-01-07T18:28:30.530113Z",
          "iopub.status.idle": "2024-01-07T18:28:44.356845Z",
          "shell.execute_reply.started": "2024-01-07T18:28:30.530086Z",
          "shell.execute_reply": "2024-01-07T18:28:44.354791Z"
        },
        "trusted": true,
        "id": "L51eekQy8WYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"display: flex; align-items: center; justify-content: center; border-radius: 10px; padding: 20px; background-color: #ffe4de; font-size: 120%; text-align: center;\">\n",
        "    <strong>\n",
        "        üöß\n",
        "Discover the full project details and application demo in the\n",
        "        <a href=\"https://github.com/FarzadNekouee/YOLOv8_Pothole_Segmentation_Road_Damage_Assessment\" style=\"color: #ed2f00; text-decoration: none;\">\n",
        "            <em><u>GitHub Repository</u></em>\n",
        "        </a>\n",
        "        üöß\n",
        "    </strong>\n",
        "</div>"
      ],
      "metadata": {
        "id": "5GjxF8Ou8WYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 align=\"left\"><font color=#ed2f00>Best Regards</font></h2>"
      ],
      "metadata": {
        "id": "JevDVipK8WYi"
      }
    }
  ]
}